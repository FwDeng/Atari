{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from atari_environment import AtariEnvironment\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "from keras import backend as K\n",
    "from model import build_network\n",
    "from globals import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enter game id\n",
    "flags.DEFINE_string('game', 'Pong-v0', 'Name of the atari game to play.')\n",
    "#to save checkpoints\n",
    "flags.DEFINE_string('experiment', 'pong', 'Name of the current game exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_final_epsilon():\n",
    "    \"\"\"\n",
    "    Annealing\n",
    "    These values are specified in section 5.1 of http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "    \"\"\"\n",
    "    final_epsilons = np.array([.1,.01,.5])\n",
    "    probabilities = np.array([0.4,0.3,0.3])\n",
    "    return np.random.choice(final_epsilons, 1, p=list(probabilities))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-25 12:30:00,864] Making new env: Pong-v0\n",
      "[2017-01-25 12:30:02,010] Making new env: Pong-v0\n",
      "[2017-01-25 12:30:02,045] Making new env: Pong-v0\n",
      "[2017-01-25 12:30:02,078] Making new env: Pong-v0\n",
      "[2017-01-25 12:30:02,112] Making new env: Pong-v0\n",
      "[2017-01-25 12:30:02,141] Making new env: Pong-v0\n",
      "[2017-01-25 12:30:02,175] Making new env: Pong-v0\n",
      "[2017-01-25 12:30:02,204] Making new env: Pong-v0\n",
      "[2017-01-25 12:30:02,244] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing workaround for pong or breakout\n",
      "Doing workaround for pong or breakout\n",
      " Starting thread Starting thread Doing workaround for pong or breakout Doing workaround for pong or breakoutDoing workaround for pong or breakout \n",
      "0\n",
      "\n",
      "1Starting thread  Starting thread Starting thread Doing workaround for pong or breakoutDoing workaround for pong or breakoutDoing workaround for pong or breakout  with final epsilon   \n",
      "\n",
      "\n",
      "with final epsilon 2 34Starting thread Starting thread Starting thread   0.5     0.5with final epsilon \n",
      "with final epsilon with final epsilon 567\n",
      "      with final epsilon 0.5with final epsilon 0.010.1with final epsilon  \n",
      " \n",
      "\n",
      " 0.010.10.5\n",
      "\n",
      "\n",
      "THREAD: 1 / TIME 9213 / TIMESTEP 1280 / EPSILON 0.99936 / REWARD -21.0 / Q_MAX 0.0659 / EPSILON PROGRESS 0.00128\n",
      "THREAD: 3 / TIME 9369 / TIMESTEP 1169 / EPSILON 0.99884269 / REWARD -20.0 / Q_MAX 0.0596 / EPSILON PROGRESS 0.001169\n",
      "THREAD: 0 / TIME 9945 / TIMESTEP 1422 / EPSILON 0.999289 / REWARD -18.0 / Q_MAX 0.0706 / EPSILON PROGRESS 0.001422\n",
      "THREAD: 5 / TIME 9948 / TIMESTEP 1181 / EPSILON 0.9989371 / REWARD -20.0 / Q_MAX 0.0582 / EPSILON PROGRESS 0.001181\n",
      "THREAD: 6 / TIME 10231 / TIMESTEP 1174 / EPSILON 0.99883774 / REWARD -21.0 / Q_MAX 0.0574 / EPSILON PROGRESS 0.001174\n",
      "THREAD: 2 / TIME 10272 / TIMESTEP 1354 / EPSILON 0.999323 / REWARD -20.0 / Q_MAX 0.0622 / EPSILON PROGRESS 0.001354\n",
      "THREAD: 4 / TIME 11726 / TIMESTEP 1384 / EPSILON 0.9987544 / REWARD -20.0 / Q_MAX 0.0557 / EPSILON PROGRESS 0.001384\n",
      "THREAD: 7 / TIME 15602 / TIMESTEP 1816 / EPSILON 0.999092 / REWARD -19.0 / Q_MAX 0.0459 / EPSILON PROGRESS 0.001816\n",
      "THREAD: 1 / TIME 18287 / TIMESTEP 2405 / EPSILON 0.9987975 / REWARD -21.0 / Q_MAX 0.0244 / EPSILON PROGRESS 0.002405\n",
      "THREAD: 5 / TIME 18438 / TIMESTEP 2226 / EPSILON 0.9979966 / REWARD -21.0 / Q_MAX 0.0215 / EPSILON PROGRESS 0.002226\n",
      "THREAD: 3 / TIME 18731 / TIMESTEP 2344 / EPSILON 0.99767944 / REWARD -21.0 / Q_MAX 0.0235 / EPSILON PROGRESS 0.002344\n",
      "THREAD: 0 / TIME 18810 / TIMESTEP 2537 / EPSILON 0.9987315 / REWARD -21.0 / Q_MAX 0.0221 / EPSILON PROGRESS 0.002537\n",
      "THREAD: 2 / TIME 19233 / TIMESTEP 2469 / EPSILON 0.9987655 / REWARD -21.0 / Q_MAX 0.0214 / EPSILON PROGRESS 0.002469\n",
      "THREAD: 6 / TIME 20865 / TIMESTEP 2492 / EPSILON 0.99753292 / REWARD -20.0 / Q_MAX 0.0203 / EPSILON PROGRESS 0.002492\n",
      "THREAD: 4 / TIME 22868 / TIMESTEP 2776 / EPSILON 0.9975016 / REWARD -20.0 / Q_MAX 0.0156 / EPSILON PROGRESS 0.002776\n"
     ]
    }
   ],
   "source": [
    "def actor_learner_thread(thread_id, env, session, graph_ops, num_actions, summary_ops, saver):\n",
    "    \"\"\"\n",
    "    Multiple Actor-learner thread for asynchronous one-step Q-learning\n",
    "    \"\"\"\n",
    "    global TMAX, T\n",
    "\n",
    "    # Unpack graph ops\n",
    "    s = graph_ops[\"s\"]\n",
    "    q_values = graph_ops[\"q_values\"]\n",
    "    st = graph_ops[\"st\"]\n",
    "    target_q_values = graph_ops[\"target_q_values\"]\n",
    "    reset_target_network_params = graph_ops[\"reset_target_network_params\"]\n",
    "    a = graph_ops[\"a\"]\n",
    "    y = graph_ops[\"y\"]\n",
    "    grad_update = graph_ops[\"grad_update\"]\n",
    "\n",
    "    summary_placeholders, update_ops, summary_op = summary_ops\n",
    "\n",
    "    # Wrap env with AtariEnvironment helper class\n",
    "    env = AtariEnvironment(gym_env=env, resized_width=FLAGS.resized_width, resized_height=FLAGS.resized_height, agent_history_length=FLAGS.agent_history_length)\n",
    "\n",
    "    # Initialize network gradients\n",
    "    s_batch = []\n",
    "    a_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    final_epsilon = sample_final_epsilon()\n",
    "    initial_epsilon = 1.0\n",
    "    epsilon = 1.0\n",
    "\n",
    "    print \"Starting thread \", thread_id, \"with final epsilon \", final_epsilon\n",
    "\n",
    "    time.sleep(3*thread_id)\n",
    "    t = 0\n",
    "    while T < TMAX:\n",
    "        # Get initial game observation\n",
    "        s_t = env.get_initial_state()\n",
    "        terminal = False\n",
    "\n",
    "        # Set up per-episode counters\n",
    "        ep_reward = 0\n",
    "        episode_ave_max_q = 0\n",
    "        ep_t = 0\n",
    "\n",
    "        while True:\n",
    "            # Forward the deep q network, get Q(s,a) values\n",
    "            readout_t = q_values.eval(session = session, feed_dict = {s : [s_t]})\n",
    "            \n",
    "            # Choose next action based on e-greedy policy\n",
    "            a_t = np.zeros([num_actions])\n",
    "            action_index = 0\n",
    "            if random.random() <= epsilon:\n",
    "                action_index = random.randrange(num_actions)\n",
    "            else:\n",
    "                action_index = np.argmax(readout_t)\n",
    "            a_t[action_index] = 1\n",
    "\n",
    "            # Scale down epsilon\n",
    "            if epsilon > final_epsilon:\n",
    "                epsilon -= (initial_epsilon - final_epsilon) / FLAGS.anneal_epsilon_timesteps\n",
    "    \n",
    "            # Gym excecutes action in game environment on behalf of actor-learner\n",
    "            s_t1, r_t, terminal, info = env.step(action_index)\n",
    "\n",
    "            # Accumulate gradients\n",
    "            readout_j1 = target_q_values.eval(session = session, feed_dict = {st : [s_t1]})\n",
    "            clipped_r_t = np.clip(r_t, -1, 1)\n",
    "            if terminal:\n",
    "                y_batch.append(clipped_r_t)\n",
    "            else:\n",
    "                y_batch.append(clipped_r_t + FLAGS.gamma * np.max(readout_j1))\n",
    "    \n",
    "            a_batch.append(a_t)\n",
    "            s_batch.append(s_t)\n",
    "    \n",
    "            # Update the state and counters\n",
    "            s_t = s_t1\n",
    "            T += 1\n",
    "            t += 1\n",
    "\n",
    "            ep_t += 1\n",
    "            ep_reward += r_t\n",
    "            episode_ave_max_q += np.max(readout_t)\n",
    "\n",
    "            # Optionally update target network\n",
    "            if T % FLAGS.target_network_update_frequency == 0:\n",
    "                session.run(reset_target_network_params)\n",
    "    \n",
    "            # Optionally update online network\n",
    "            if t % FLAGS.network_update_frequency == 0 or terminal:\n",
    "                if s_batch:\n",
    "                    session.run(grad_update, feed_dict = {y : y_batch,\n",
    "                                                          a : a_batch,\n",
    "                                                          s : s_batch})\n",
    "                # Clear gradients\n",
    "                s_batch = []\n",
    "                a_batch = []\n",
    "                y_batch = []\n",
    "    \n",
    "            # Save model progress\n",
    "            if t % FLAGS.checkpoint_interval == 0:\n",
    "                saver.save(session, FLAGS.checkpoint_dir+\"/\"+FLAGS.experiment+\".ckpt\", global_step = t)\n",
    "    \n",
    "            # Print end of episode stats\n",
    "            if terminal:\n",
    "                stats = [ep_reward, episode_ave_max_q/float(ep_t), epsilon]\n",
    "                for i in range(len(stats)):\n",
    "                    session.run(update_ops[i], feed_dict={summary_placeholders[i]:float(stats[i])})\n",
    "                print \"THREAD:\", thread_id, \"/ TIME\", T, \"/ TIMESTEP\", t, \"/ EPSILON\", epsilon, \"/ REWARD\", ep_reward, \"/ Q_MAX %.4f\" % (episode_ave_max_q/float(ep_t)), \"/ EPSILON PROGRESS\", t/float(FLAGS.anneal_epsilon_timesteps)\n",
    "                break\n",
    "\n",
    "def build_graph(num_actions):\n",
    "    # Create shared deep q network\n",
    "    s, q_network = build_network(num_actions=num_actions, agent_history_length=FLAGS.agent_history_length, resized_width=FLAGS.resized_width, resized_height=FLAGS.resized_height)\n",
    "    network_params = q_network.trainable_weights\n",
    "    q_values = q_network(s)\n",
    "\n",
    "    # Create shared target network\n",
    "    st, target_q_network = build_network(num_actions=num_actions, agent_history_length=FLAGS.agent_history_length, resized_width=FLAGS.resized_width, resized_height=FLAGS.resized_height)\n",
    "    target_network_params = target_q_network.trainable_weights\n",
    "    target_q_values = target_q_network(st)\n",
    "\n",
    "    # Op for periodically updating target network with online network weights\n",
    "    reset_target_network_params = [target_network_params[i].assign(network_params[i]) for i in range(len(target_network_params))]\n",
    "    \n",
    "    # Define cost and gradient update op\n",
    "    a = tf.placeholder(\"float\", [None, num_actions])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "    action_q_values = tf.reduce_sum(tf.mul(q_values, a), reduction_indices=1)\n",
    "    cost = tf.reduce_mean(tf.square(y - action_q_values))\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "    grad_update = optimizer.minimize(cost, var_list=network_params)\n",
    "\n",
    "    graph_ops = {\"s\" : s, \n",
    "                 \"q_values\" : q_values,\n",
    "                 \"st\" : st, \n",
    "                 \"target_q_values\" : target_q_values,\n",
    "                 \"reset_target_network_params\" : reset_target_network_params,\n",
    "                 \"a\" : a,\n",
    "                 \"y\" : y,\n",
    "                 \"grad_update\" : grad_update}\n",
    "\n",
    "    return graph_ops\n",
    "\n",
    "# Set up some episode summary ops to visualize on tensorboard.\n",
    "def setup_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.scalar_summary(\"Episode Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.scalar_summary(\"Max Q Value\", episode_ave_max_q)\n",
    "    logged_epsilon = tf.Variable(0.)\n",
    "    tf.scalar_summary(\"Epsilon\", logged_epsilon)\n",
    "    logged_T = tf.Variable(0.)\n",
    "    summary_vars = [episode_reward, episode_ave_max_q, logged_epsilon]\n",
    "    summary_placeholders = [tf.placeholder(\"float\") for i in range(len(summary_vars))]\n",
    "    update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    return summary_placeholders, update_ops, summary_op\n",
    "\n",
    "def get_num_actions():\n",
    "    \"\"\"\n",
    "    Returns the number of possible actions for the given atari game\n",
    "    \"\"\"\n",
    "    # Figure out number of actions from gym env\n",
    "    env = gym.make(FLAGS.game)\n",
    "    num_actions = env.action_space.n\n",
    "    if (FLAGS.game == \"Pong-v0\" or FLAGS.game == \"Breakout-v0\"):\n",
    "        num_actions = 3\n",
    "    return num_actions\n",
    "\n",
    "def train(session, graph_ops, num_actions, saver):\n",
    "    # Initialize target network weights\n",
    "    session.run(graph_ops[\"reset_target_network_params\"])\n",
    "\n",
    "    # Set up game environments (one per thread)\n",
    "    envs = [gym.make(FLAGS.game) for i in range(FLAGS.num_concurrent)]\n",
    "    \n",
    "    summary_ops = setup_summaries()\n",
    "    summary_op = summary_ops[-1]\n",
    "\n",
    "    # Initialize variables\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    summary_save_path = FLAGS.summary_dir + \"/\" + FLAGS.experiment\n",
    "    writer = tf.train.SummaryWriter(summary_save_path, session.graph)\n",
    "    if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "        os.makedirs(FLAGS.checkpoint_dir)\n",
    "\n",
    "    # Start num_concurrent actor-learner training threads\n",
    "    actor_learner_threads = [threading.Thread(target=actor_learner_thread, args=(thread_id, envs[thread_id], session, graph_ops, num_actions, summary_ops, saver)) for thread_id in range(FLAGS.num_concurrent)]\n",
    "    for t in actor_learner_threads:\n",
    "        t.start()\n",
    "\n",
    "    # Show the agents training and write summary statistics\n",
    "    last_summary_time = 0\n",
    "    while True:\n",
    "        if FLAGS.show_training:\n",
    "            for env in envs:\n",
    "                env.render()\n",
    "        now = time.time()\n",
    "        if now - last_summary_time > FLAGS.summary_interval:\n",
    "            summary_str = session.run(summary_op)\n",
    "            writer.add_summary(summary_str, float(T))\n",
    "            last_summary_time = now\n",
    "    for t in actor_learner_threads:\n",
    "        t.join()\n",
    "\n",
    "def evaluation(session, graph_ops, saver):\n",
    "    saver.restore(session, FLAGS.checkpoint_path)\n",
    "    print \"Restored model weights from \", FLAGS.checkpoint_path\n",
    "    monitor_env = gym.make(FLAGS.game)\n",
    "    monitor_env.monitor.start(FLAGS.eval_dir+\"/\"+FLAGS.experiment+\"/eval\")\n",
    "\n",
    "    # Unpack graph ops\n",
    "    s = graph_ops[\"s\"]\n",
    "    q_values = graph_ops[\"q_values\"]\n",
    "\n",
    "    # Wrap env with AtariEnvironment helper class\n",
    "    env = AtariEnvironment(gym_env=monitor_env, resized_width=FLAGS.resized_width, resized_height=FLAGS.resized_height, agent_history_length=FLAGS.agent_history_length)\n",
    "\n",
    "    for i_episode in xrange(FLAGS.num_eval_episodes):\n",
    "        s_t = env.get_initial_state()\n",
    "        ep_reward = 0\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            monitor_env.render()\n",
    "            readout_t = q_values.eval(session = session, feed_dict = {s : [s_t]})\n",
    "            action_index = np.argmax(readout_t)\n",
    "            s_t1, r_t, terminal, info = env.step(action_index)\n",
    "            s_t = s_t1\n",
    "            ep_reward += r_t\n",
    "        print ep_reward\n",
    "    monitor_env.monitor.close()\n",
    "\n",
    "def main(_):\n",
    "  g = tf.Graph()\n",
    "  with g.as_default(), tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    num_actions = get_num_actions()\n",
    "    graph_ops = build_graph(num_actions)\n",
    "    saver = tf.train.Saver()\n",
    "    train(session, graph_ops, num_actions, saver)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
